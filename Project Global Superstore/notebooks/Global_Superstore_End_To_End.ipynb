{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee49465c",
   "metadata": {},
   "source": [
    "# Global Superstore — End-to-End Analytics (Portfolio Edition)\n",
    "\n",
    "**What this shows recruiters:** a disciplined pipeline from **raw Excel → cleaned fact/dims → DQ tests → EDA → SQL → cohorts/RFM → exports for BI**, with narrative and guardrails. Each section explains the why + how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47edb57",
   "metadata": {},
   "source": [
    "## 0) Project framing — objectives & business questions\n",
    "\n",
    "**Why this exists:** reviewers should immediately see *what decisions this analysis enables* and *how the notebook is structured*.\n",
    "\n",
    "**Objectives**\n",
    "- Build a reliable, explainable dataset for sales analytics (clean types, consistent keys).\n",
    "- Quantify performance by **Category** and **Market**; validate operational metrics.\n",
    "- Segment customers (**RFM**) and analyze **cohort retention**.\n",
    "- Export a **star schema** (fact + dims) for Power BI.\n",
    "\n",
    "**Guiding questions**\n",
    "1) Which **categories/markets** drive sales and profit?  \n",
    "2) How do **discounts** relate to **profit margin**?  \n",
    "3) What’s our **return rate** by category/market?  \n",
    "4) Are we **shipping on time**? What are typical lead times (mean / p95)?  \n",
    "5) Which **customer segments** are most valuable? Do they retain?\n",
    "\n",
    "> **Portfolio note:** This section helps interviewers scan your work. Keep it lean but concrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af36f5",
   "metadata": {},
   "source": [
    "## 1) Imports, paths & robust Excel helpers\n",
    "\n",
    "**What we do**\n",
    "- Import the analytics stack (`pandas`, `numpy`, `duckdb`, `matplotlib`, `plotly`).\n",
    "- Create a small `Paths` dataclass to keep folder structure reproducible (`data/raw`, `data/processed`, `reports`).\n",
    "- Define helpers:\n",
    "  - `standardize_columns(df)`: trims/collapses spaces and *preserves* canonical column names we expect.\n",
    "  - `read_sheet_any_case(path, name)`: reads a sheet even if the tab name varies in case or contains extra words.\n",
    "\n",
    "**Why it matters**\n",
    "- Prevents brittle failures when tab names change.\n",
    "- Makes the notebook portable across machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import re, math, warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "@dataclass\n",
    "class Paths:\n",
    "    root: Path = Path(\".\")\n",
    "    raw: Path = Path(\"data/raw\")\n",
    "    processed: Path = Path(\"data/processed\")\n",
    "    reports: Path = Path(\"reports\")\n",
    "\n",
    "paths = Paths()\n",
    "for d in (paths.raw, paths.processed, paths.reports):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXCEL_FILE = paths.raw / \"GlobalSuperstore.xls\"\n",
    "print(\"Expecting Excel at:\", EXCEL_FILE.resolve())\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep = {\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Customer ID\",\"Customer Name\",\n",
    "            \"Segment\",\"City\",\"State\",\"Country\",\"Postal Code\",\"Market\",\"Region\",\"Product ID\",\n",
    "            \"Category\",\"Sub-Category\",\"Product Name\",\"Sales\",\"Quantity\",\"Discount\",\"Profit\",\n",
    "            \"Shipping Cost\",\"Order Priority\",\"Returned\",\"Person\"}\n",
    "    def clean(col: str) -> str:\n",
    "        col = re.sub(r\"\\s+\",\" \",str(col).strip())\n",
    "        if col in keep: return col\n",
    "        return \" \".join(w.capitalize() for w in col.split(\" \"))\n",
    "    out = df.copy()\n",
    "    out.columns = [clean(c) for c in out.columns]\n",
    "    return out\n",
    "\n",
    "def read_sheet_any_case(path: Path, name: str) -> pd.DataFrame:\n",
    "    xls = pd.ExcelFile(path)\n",
    "    target = name.lower()\n",
    "    for n in xls.sheet_names:\n",
    "        if n.lower() == target:\n",
    "            return pd.read_excel(path, sheet_name=n)\n",
    "    for n in xls.sheet_names:\n",
    "        if target in n.lower():\n",
    "            return pd.read_excel(path, sheet_name=n)\n",
    "    raise ValueError(f\"Sheet '{name}' not found. Available: {xls.sheet_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d1917",
   "metadata": {},
   "source": [
    "## 2) Data intake & contracts (schema checks)\n",
    "**Why:** Fail fast on missing columns or bad joins; make the pipeline **idempotent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Load raw sheets\n",
    "orders  = standardize_columns(read_sheet_any_case(EXCEL_FILE, \"Orders\"))\n",
    "returns = standardize_columns(read_sheet_any_case(EXCEL_FILE, \"Returns\"))\n",
    "people  = standardize_columns(read_sheet_any_case(EXCEL_FILE, \"People\"))\n",
    "\n",
    "# Contracts: required columns\n",
    "req_orders = {\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Customer ID\",\"Customer Name\",\"Segment\",\n",
    "              \"City\",\"State\",\"Country\",\"Market\",\"Region\",\"Product ID\",\"Category\",\"Sub-Category\",\n",
    "              \"Product Name\",\"Sales\",\"Quantity\",\"Discount\",\"Profit\",\"Shipping Cost\",\"Order Priority\"}\n",
    "missing = req_orders - set(orders.columns)\n",
    "assert not missing, f\"Orders missing columns: {missing}\"\n",
    "assert {\"Order ID\",\"Returned\",\"Market\"}.issubset(returns.columns), \"Returns missing columns\"\n",
    "assert {\"Person\",\"Region\"}.issubset(people.columns), \"People missing columns\"\n",
    "\n",
    "# Normalize keys/dtypes\n",
    "orders[\"Order ID\"]  = orders[\"Order ID\"].astype(str).str.strip()\n",
    "returns[\"Order ID\"] = returns[\"Order ID\"].astype(str).str.strip()\n",
    "returns[\"Returned\"] = (returns[\"Returned\"].astype(str).str.strip().str.lower()\n",
    "                       .map({\"yes\": True, \"y\": True, \"true\": True, \"1\": True,\n",
    "                             \"no\": False, \"n\": False, \"false\": False, \"0\": False}))\n",
    "\n",
    "# Harmonize Markets in Returns\n",
    "market_map = {\n",
    "    \"United States\":\"US\",\"U.S.\":\"US\",\"USA\":\"US\",\"Usa\":\"US\",\"us\":\"US\",\n",
    "    \"Europe\":\"EU\",\"European Union\":\"EU\",\n",
    "    \"Asia Pacific\":\"APAC\",\"ASIA PACIFIC\":\"APAC\",\n",
    "    \"Latin America\":\"LATAM\",\n",
    "    \"US\":\"US\",\"EU\":\"EU\",\"APAC\":\"APAC\",\"LATAM\":\"LATAM\",\"EMEA\":\"EMEA\",\"Africa\":\"Africa\",\"Canada\":\"Canada\"\n",
    "}\n",
    "returns[\"Market\"] = returns[\"Market\"].astype(str).str.strip()\n",
    "returns[\"Market_std\"] = returns[\"Market\"].map(lambda x: market_map.get(x, x))\n",
    "\n",
    "# Harmonize Regions in People\n",
    "people[\"Region\"] = people[\"Region\"].astype(str).str.strip()\n",
    "people[\"Region_std\"] = people[\"Region\"].replace({\"AMEA\":\"EMEA\",\"emea\":\"EMEA\",\"E.M.E.A\":\"EMEA\"})\n",
    "\n",
    "# QA: referential integrity & market mismatches\n",
    "ret_not_in_orders = set(returns[\"Order ID\"]) - set(orders[\"Order ID\"])\n",
    "print(\"Returns not in Orders:\", len(ret_not_in_orders))\n",
    "\n",
    "r = returns[[\"Order ID\",\"Market_std\"]].drop_duplicates(\"Order ID\")\n",
    "o = orders[[\"Order ID\",\"Market\"]].drop_duplicates(\"Order ID\")\n",
    "mk = o.merge(r, on=\"Order ID\", how=\"left\")\n",
    "mk[\"match\"] = (mk[\"Market\"] == mk[\"Market_std\"]) | mk[\"Market_std\"].isna()\n",
    "print(\"Market mismatches (pre):\", int((~mk[\"match\"]).sum()))\n",
    "\n",
    "# Canonicalize Returns to Orders' market\n",
    "market_map_orders = o.set_index(\"Order ID\")[\"Market\"]\n",
    "returns[\"Market_std\"] = returns[\"Order ID\"].map(market_map_orders).fillna(returns[\"Market_std\"])\n",
    "\n",
    "# Re-check\n",
    "r2 = returns[[\"Order ID\",\"Market_std\"]].drop_duplicates(\"Order ID\")\n",
    "mk2 = o.merge(r2, on=\"Order ID\", how=\"left\")\n",
    "mk2[\"match\"] = (mk2[\"Market\"] == mk2[\"Market_std\"]) | mk2[\"Market_std\"].isna()\n",
    "print(\"Market mismatches (post):\", int((~mk2[\"match\"]).sum()))\n",
    "\n",
    "# Returned flag (idempotent) + Sales rep by Region\n",
    "orders = orders.drop(columns=[c for c in orders.columns if c.lower().startswith(\"returned\")], errors=\"ignore\")\n",
    "ret_flag = returns[[\"Order ID\",\"Returned\"]].drop_duplicates(\"Order ID\")\n",
    "orders = orders.merge(ret_flag, on=\"Order ID\", how=\"left\", validate=\"m:1\", suffixes=(\"\", \"_ret\"))\n",
    "if \"Returned_ret\" in orders.columns and \"Returned\" not in orders.columns:\n",
    "    orders = orders.rename(columns={\"Returned_ret\":\"Returned\"})\n",
    "orders[\"Returned\"] = orders[\"Returned\"].astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "people_clean = (people[[\"Region_std\",\"Person\"]].rename(columns={\"Region_std\":\"Region\"})\n",
    "                .assign(Region=lambda d: d[\"Region\"].astype(str).str.strip())\n",
    "                .sort_values([\"Region\",\"Person\"]).drop_duplicates(\"Region\"))\n",
    "orders[\"Region\"] = orders[\"Region\"].astype(str).str.strip()\n",
    "orders = orders.drop(columns=[c for c in orders.columns if c.lower().startswith(\"person\")], errors=\"ignore\")\n",
    "orders = orders.merge(people_clean, on=\"Region\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "display(orders.head(3)); display(returns.head(3)); display(people.head(3))\n",
    "print({\n",
    "    \"orders_rows\": len(orders),\n",
    "    \"returns_rows\": len(returns),\n",
    "    \"distinct_orders\": orders[\"Order ID\"].nunique(),\n",
    "    \"returned_lines\": int(orders[\"Returned\"].sum()),\n",
    "    \"no_rep_rows\": int(orders[\"Person\"].isna().sum())\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86631ca",
   "metadata": {},
   "source": [
    "## 3) Cleaning & feature engineering (gold fact table)\n",
    "\n",
    "**Types & categoricals**\n",
    "- Parse date columns; coerce numeric columns (Sales/Profit/Discount/Shipping Cost/Quantity).\n",
    "- Store `Postal Code` as **string** to preserve leading zeros and support non‑US formats.\n",
    "- Normalize strings (trim spaces; Title Case for City/State).\n",
    "\n",
    "**Deduplication**\n",
    "- Prefer unique `Row ID` when available; otherwise use a composite (`Order ID`, `Product ID`, `Order Date`, `Customer ID`).\n",
    "\n",
    "**Guardrails**\n",
    "- Negative `Quantity` → absolute value (data capture issues).  \n",
    "- Clip `Discount` to `[0,1]` to avoid invalid math.\n",
    "\n",
    "**Derived fields**\n",
    "- `Ship Lead Time (days)` = `Ship Date` − `Order Date`  \n",
    "- `Year`, `Month` (timestamp for monthly trending)  \n",
    "- `Profit Margin` = `Profit / Sales` (safe with zeros → NaN)\n",
    "\n",
    "**Missingness snapshot**\n",
    "- Display null% by column to justify imputation decisions (e.g., Postal Code often ~80% missing outside US).\n",
    "\n",
    "> **Portfolio note:** Explain *why* each guardrail exists (e.g., to stabilize metrics / avoid divide‑by‑zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19beae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = orders.copy()\n",
    "\n",
    "# Types\n",
    "for c in [\"Order Date\",\"Ship Date\"]:\n",
    "    df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "for c in [\"Sales\",\"Profit\",\"Discount\",\"Shipping Cost\",\"Quantity\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "if \"Postal Code\" in df.columns:\n",
    "    df[\"Postal Code\"] = df[\"Postal Code\"].astype(\"string\")\n",
    "\n",
    "# Categoricals\n",
    "for c in [\"Ship Mode\",\"Segment\",\"City\",\"State\",\"Country\",\"Market\",\"Region\",\n",
    "          \"Category\",\"Sub-Category\",\"Order Priority\",\"Customer Name\",\"Person\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\").str.strip().str.replace(r\"\\s+\",\" \", regex=True)\n",
    "for c in [\"City\",\"State\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].str.title()\n",
    "\n",
    "# Deduplicate\n",
    "before = len(df)\n",
    "if \"Row ID\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"Row ID\"])\n",
    "else:\n",
    "    subset = [x for x in [\"Order ID\",\"Product ID\",\"Order Date\",\"Customer ID\"] if x in df.columns]\n",
    "    df = df.drop_duplicates(subset=subset)\n",
    "print(\"Deduped rows:\", before - len(df))\n",
    "\n",
    "# Guardrails\n",
    "neg_qty = df[\"Quantity\"] < 0\n",
    "if neg_qty.any():\n",
    "    df.loc[neg_qty, \"Quantity\"] = df.loc[neg_qty, \"Quantity\"].abs()\n",
    "if \"Discount\" in df.columns:\n",
    "    df[\"Discount\"] = df[\"Discount\"].clip(0,1)\n",
    "\n",
    "# Features\n",
    "df[\"Ship Lead Time (days)\"] = (df[\"Ship Date\"] - df[\"Order Date\"]).dt.days\n",
    "df[\"Year\"]  = df[\"Order Date\"].dt.year\n",
    "df[\"Month\"] = df[\"Order Date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "df[\"Profit Margin\"] = np.where(df[\"Sales\"].ne(0), df[\"Profit\"]/df[\"Sales\"], np.nan)\n",
    "\n",
    "# Missingness snapshot\n",
    "nulls = df.isna().mean().sort_values(ascending=False)\n",
    "display(nulls.head(12))\n",
    "\n",
    "orders_clean = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8f330",
   "metadata": {},
   "source": [
    "## 4) Data-quality flags + viz-friendly copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def robust_group_outliers(frame, col, by=[\"Market\",\"Category\"], z=4.5, min_group=30):\n",
    "    g = frame.groupby(by, dropna=False)\n",
    "    out = pd.Series(False, index=frame.index)\n",
    "    for _, part in g:\n",
    "        if len(part) < min_group: \n",
    "            continue\n",
    "        x = pd.to_numeric(part[col], errors=\"coerce\")\n",
    "        med = x.median()\n",
    "        mad = np.median(np.abs(x - med))\n",
    "        if mad == 0 or np.isnan(mad): \n",
    "            continue\n",
    "        zrob = 0.6745 * (x - med) / mad\n",
    "        out.loc[part.index] = zrob.abs() > z\n",
    "    return out\n",
    "\n",
    "dq_flags = pd.DataFrame(index=orders_clean.index)\n",
    "for c in [\"Sales\",\"Profit\",\"Shipping Cost\"]:\n",
    "    dq_flags[f\"{c}_outlier\"] = robust_group_outliers(orders_clean, c)\n",
    "dq_flags[\"neg_lead\"]  = orders_clean[\"Ship Lead Time (days)\"] < 0\n",
    "lt_99 = orders_clean[\"Ship Lead Time (days)\"].quantile(0.99)\n",
    "dq_flags[\"long_lead\"] = orders_clean[\"Ship Lead Time (days)\"] > lt_99\n",
    "display(dq_flags.sum().to_frame(\"count\").T)\n",
    "\n",
    "def winsorize_group(frame, col, by=[\"Market\",\"Category\"], q_low=0.005, q_high=0.995, min_group=30):\n",
    "    x = frame[col].copy()\n",
    "    g = frame.groupby(by, dropna=False)\n",
    "    for _, part in g:\n",
    "        if len(part) < min_group: \n",
    "            continue\n",
    "        lo = part[col].quantile(q_low)\n",
    "        hi = part[col].quantile(q_high)\n",
    "        x.loc[part.index] = part[col].clip(lo, hi)\n",
    "    return x\n",
    "\n",
    "orders_viz = orders_clean.copy()\n",
    "for c in [\"Sales\",\"Profit\",\"Shipping Cost\"]:\n",
    "    orders_viz[c] = winsorize_group(orders_viz, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfc472",
   "metadata": {},
   "source": [
    "## 5) Missing-value strategy (hierarchical, only when needed)\n",
    "\n",
    "**Approach**\n",
    "- Fill `Shipping Cost` / `Discount` via hierarchical medians: (`Ship Mode`,`Market`) → (`Ship Mode`) → global.  \n",
    "- If `Ship Date` is missing, impute as `Order Date + median lead` for that (`Ship Mode`,`Market`). Recompute lead.\n",
    "\n",
    "**Policy on Postal Code**\n",
    "- Leave nulls as-is (very high missingness outside US). Do **not** fabricate postal codes.\n",
    "\n",
    "**Auditability**\n",
    "- Create `_imputed` flags so we can quantify where imputations were applied (transparency for stakeholders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean = orders_clean.copy()\n",
    "\n",
    "def fill_by_hierarchy(df, value_col, levels):\n",
    "    out = df[value_col].copy()\n",
    "    imputed_from_null = out.isna()\n",
    "    for groups in levels:\n",
    "        if not imputed_from_null.any(): \n",
    "            break\n",
    "        if groups:\n",
    "            med = df.groupby(groups, dropna=False)[value_col].median()\n",
    "            idx = imputed_from_null\n",
    "            out.loc[idx] = df.loc[idx, groups].apply(lambda r: med.get(tuple(r.values), np.nan), axis=1)\n",
    "            imputed_from_null = out.isna()\n",
    "        else:\n",
    "            out.loc[imputed_from_null] = df[value_col].median()\n",
    "            imputed_from_null = out.isna()\n",
    "    flag = df[value_col].isna() & out.notna()\n",
    "    return out, flag\n",
    "\n",
    "for col in [\"Shipping Cost\",\"Discount\"]:\n",
    "    if col in clean.columns and clean[col].isna().any():\n",
    "        filled, flag = fill_by_hierarchy(clean, col, [[\"Ship Mode\",\"Market\"],[\"Ship Mode\"],[]])\n",
    "        clean[col] = filled.clip(0,1) if col==\"Discount\" else filled\n",
    "        clean[f\"{col.replace(' ','')}_imputed\"] = False\n",
    "        clean.loc[flag, f\"{col.replace(' ','')}_imputed\"] = True\n",
    "\n",
    "ship_null = clean[\"Ship Date\"].isna()\n",
    "if ship_null.any():\n",
    "    med_lead = (clean.assign(lead=(clean[\"Ship Date\"]-clean[\"Order Date\"]).dt.days)\n",
    "                      .groupby([\"Ship Mode\",\"Market\"], dropna=False)[\"lead\"].median().to_dict())\n",
    "    def impute_sd(r):\n",
    "        if pd.isna(r[\"Ship Date\"]) and pd.notna(r[\"Order Date\"]):\n",
    "            val = med_lead.get((r.get(\"Ship Mode\", np.nan), r.get(\"Market\", np.nan)), np.nan)\n",
    "            if pd.notna(val):\n",
    "                return r[\"Order Date\"] + pd.Timedelta(days=int(val))\n",
    "        return r[\"Ship Date\"]\n",
    "    clean.loc[ship_null,\"Ship Date\"] = clean.loc[ship_null].apply(impute_sd, axis=1)\n",
    "    clean[\"Ship Lead Time (days)\"] = (clean[\"Ship Date\"] - clean[\"Order Date\"]).dt.days\n",
    "    clean[\"ShipDate_imputed\"] = ship_null & clean[\"Ship Date\"].notna()\n",
    "\n",
    "clean[\"has_postal_code\"] = clean.get(\"Postal Code\", pd.Series(index=clean.index)).notna()\n",
    "\n",
    "orders_clean = clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69380e5",
   "metadata": {},
   "source": [
    "## 6) Tests & data dictionary\n",
    "\n",
    "**Lightweight tests**\n",
    "- No missing `Order ID`.\n",
    "- `Discount` is within `[0,1]`.\n",
    "- `Quantity` is non-negative.\n",
    "\n",
    "**Data dictionary**\n",
    "- Short, human-friendly descriptions for key fields, alongside dtypes.\n",
    "\n",
    "> **Portfolio note:** Small assertions show discipline. They’re simple to read and signal quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1022b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tests\n",
    "assert orders_clean[\"Order ID\"].isna().sum() == 0\n",
    "assert (orders_clean[\"Discount\"].between(0,1)).all()\n",
    "assert orders_clean[\"Quantity\"].ge(0).all()\n",
    "\n",
    "# Data dictionary\n",
    "descriptions = {\n",
    "    \"Order ID\":\"Order identifier (order-level)\",\n",
    "    \"Row ID\":\"Unique line identifier (line-level)\",\n",
    "    \"Order Date\":\"Date order was placed\",\n",
    "    \"Ship Date\":\"Date order shipped\",\n",
    "    \"Ship Mode\":\"Fulfillment speed/tier\",\n",
    "    \"Customer ID\":\"Customer identifier\",\n",
    "    \"Customer Name\":\"Customer full name\",\n",
    "    \"Segment\":\"Customer segment\",\n",
    "    \"City\":\"Ship-to city\",\n",
    "    \"State\":\"Ship-to state/province\",\n",
    "    \"Country\":\"Ship-to country\",\n",
    "    \"Postal Code\":\"Ship-to postal/ZIP\",\n",
    "    \"Market\":\"Market region code (US, EU, APAC, LATAM, etc.)\",\n",
    "    \"Region\":\"Sales territory (used to map sales reps)\",\n",
    "    \"Product ID\":\"Product SKU\",\n",
    "    \"Category\":\"Product category\",\n",
    "    \"Sub-Category\":\"Product sub-category\",\n",
    "    \"Product Name\":\"Product description\",\n",
    "    \"Sales\":\"Line revenue\",\n",
    "    \"Quantity\":\"Units on the line\",\n",
    "    \"Discount\":\"Fractional discount (0–1)\",\n",
    "    \"Profit\":\"Line profit (can be negative)\",\n",
    "    \"Shipping Cost\":\"Freight cost for line\",\n",
    "    \"Order Priority\":\"Order urgency flag\",\n",
    "    \"Returned\":\"True if order is in Returns tab\",\n",
    "    \"Person\":\"Sales rep assigned to region\",\n",
    "    \"Ship Lead Time (days)\":\"Days from order to ship\",\n",
    "    \"Profit Margin\":\"Profit / Sales (line-level)\",\n",
    "    \"Year\":\"Order year\",\n",
    "    \"Month\":\"Order year-month (timestamp)\",\n",
    "}\n",
    "dd = (pd.DataFrame({\n",
    "        \"column\": orders_clean.columns,\n",
    "        \"dtype\": [str(t) for t in orders_clean.dtypes],\n",
    "        \"description\": [descriptions.get(c,\"\") for c in orders_clean.columns]\n",
    "     }))\n",
    "dd.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902a88a",
   "metadata": {},
   "source": [
    "## 7) EDA — distributions & trends (uses `orders_viz`)\n",
    "\n",
    "**Visuals**\n",
    "- **Monthly Sales & Profit:** trend checks; seasonality; structural breaks.  \n",
    "- **Top Sub-Categories by Profit:** where value concentrates.  \n",
    "- **Discount vs Profit Margin:** sanity-check negative relationship; color by Category to see differing patterns.\n",
    "\n",
    "**Reading tips**\n",
    "- Use `orders_viz` for charts to avoid outliers dominating visuals.  \n",
    "- Keep KPI computations on `orders_clean` (raw) to preserve truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Monthly sales & profit\n",
    "monthly = orders_viz.groupby(\"Month\", as_index=False).agg(sales=(\"Sales\",\"sum\"), profit=(\"Profit\",\"sum\"))\n",
    "ax = monthly.plot(x=\"Month\", y=[\"sales\",\"profit\"], title=\"Monthly Sales & Profit (winsorized for viz)\")\n",
    "ax.set_xlabel(\"Month\"); ax.set_ylabel(\"Value\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Top sub-categories by profit\n",
    "top_sub = (orders_viz.groupby(\"Sub-Category\", as_index=False)\n",
    "                .agg(profit=(\"Profit\",\"sum\"))\n",
    "                .sort_values(\"profit\", ascending=False)\n",
    "                .head(15))\n",
    "fig = px.bar(top_sub, x=\"Sub-Category\", y=\"profit\", title=\"Top 15 Sub-Categories by Profit (viz)\")\n",
    "fig.update_layout(xaxis_tickangle=-35); fig.show()\n",
    "\n",
    "# Discount vs Profit Margin\n",
    "tmp = orders_viz.copy()\n",
    "tmp[\"Profit Margin\"] = np.where(tmp[\"Sales\"]!=0, tmp[\"Profit\"]/tmp[\"Sales\"], np.nan)\n",
    "fig = px.scatter(tmp, x=\"Discount\", y=\"Profit Margin\", color=\"Category\",\n",
    "                 title=\"Discount vs Profit Margin (viz)\", hover_data=[\"Sub-Category\",\"Product Name\"])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6f333",
   "metadata": {},
   "source": [
    "## 8) SQL analysis (DuckDB in-memory)\n",
    "\n",
    "**Queries shown**\n",
    "- **Category KPIs:** SUM(Sales), SUM(Profit), and Profit Margin.  \n",
    "- **Return rate** by Market × Category (uses distinct order counts to avoid line duplication).  \n",
    "- **Top product per sub-category** via window function (`ROW_NUMBER()` over profits).\n",
    "\n",
    "**Why DuckDB**\n",
    "- In-memory, zero setup; shows you’re comfortable switching between pandas and SQL idioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad00ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "con.register(\"orders\", orders_clean)\n",
    "con.register(\"returns\", returns)\n",
    "con.register(\"people\", people)\n",
    "con.execute(\"CREATE OR REPLACE VIEW v_orders  AS SELECT * FROM orders;\")\n",
    "con.execute(\"CREATE OR REPLACE VIEW v_returns AS SELECT * FROM returns;\")\n",
    "con.execute(\"CREATE OR REPLACE VIEW v_people  AS SELECT * FROM people;\")\n",
    "print(\"Views ready.\")\n",
    "\n",
    "sql_category = '''\n",
    "SELECT Category,\n",
    "       SUM(Sales)  AS Sales,\n",
    "       SUM(Profit) AS Profit,\n",
    "       CASE WHEN SUM(Sales) <> 0 THEN SUM(Profit)/SUM(Sales) END AS Profit_Margin\n",
    "FROM v_orders\n",
    "GROUP BY 1\n",
    "ORDER BY Sales DESC;\n",
    "'''\n",
    "df_category = con.execute(sql_category).df()\n",
    "df_category.head()\n",
    "\n",
    "sql_return_rate = '''\n",
    "SELECT o.Market, o.Category,\n",
    "       COUNT(DISTINCT o.\"Order ID\") AS orders,\n",
    "       COUNT(DISTINCT r.\"Order ID\") AS returned_orders,\n",
    "       COUNT(DISTINCT r.\"Order ID\") * 1.0 / NULLIF(COUNT(DISTINCT o.\"Order ID\"),0) AS return_rate\n",
    "FROM v_orders o\n",
    "LEFT JOIN v_returns r ON o.\"Order ID\" = r.\"Order ID\"\n",
    "GROUP BY 1,2\n",
    "ORDER BY return_rate DESC NULLS LAST;\n",
    "'''\n",
    "df_return_rate = con.execute(sql_return_rate).df()\n",
    "df_return_rate.head()\n",
    "\n",
    "sql_top_product = '''\n",
    "WITH prod_profit AS (\n",
    "  SELECT \"Sub-Category\", \"Product ID\", \"Product Name\",\n",
    "         SUM(Profit) AS profit\n",
    "  FROM v_orders\n",
    "  GROUP BY 1,2,3\n",
    ")\n",
    "SELECT * FROM (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY \"Sub-Category\" ORDER BY profit DESC) AS rn\n",
    "  FROM prod_profit\n",
    ")\n",
    "WHERE rn = 1\n",
    "ORDER BY profit DESC;\n",
    "'''\n",
    "df_top_product = con.execute(sql_top_product).df()\n",
    "df_top_product.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa6be5",
   "metadata": {},
   "source": [
    "## 9) Discount–margin elasticity (quick regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discount_elasticity_by_subcat(df: pd.DataFrame, min_rows:int=100, min_unique:int=5) -> pd.DataFrame:\n",
    "    out = []\n",
    "    g = df.copy()\n",
    "    g = g[g[\"Sales\"] > 0].copy()\n",
    "    g[\"margin\"] = np.where(g[\"Sales\"]!=0, g[\"Profit\"]/g[\"Sales\"], np.nan)\n",
    "    g = g.replace([np.inf,-np.inf], np.nan).dropna(subset=[\"Discount\",\"margin\"])\n",
    "\n",
    "    for subcat, part in g.groupby(\"Sub-Category\"):\n",
    "        if len(part) < min_rows or part[\"Discount\"].nunique() < min_unique:\n",
    "            continue\n",
    "        x = part[\"Discount\"].values\n",
    "        y = part[\"margin\"].values\n",
    "        a, b = np.polyfit(x, y, 1)   # slope, intercept\n",
    "        yhat = a*x + b\n",
    "        ss_res = np.sum((y - yhat)**2)\n",
    "        ss_tot = np.sum((y - y.mean())**2)\n",
    "        r2 = 1 - ss_res/ss_tot if ss_tot != 0 else np.nan\n",
    "        out.append({\"Sub-Category\": subcat, \"slope\": a, \"r2\": r2, \"n\": len(part)})\n",
    "    return (pd.DataFrame(out).sort_values([\"slope\",\"r2\"], ascending=[True, False]))\n",
    "\n",
    "elasticity = discount_elasticity_by_subcat(orders_clean, min_rows=100, min_unique=5)\n",
    "elasticity.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db839f",
   "metadata": {},
   "source": [
    "\n",
    "### 9.1) Intercepts & Break‑Even Discount (manager‑friendly table)\n",
    "\n",
    "- We extend the Step 9 fit (`margin = a·discount + b`) with:\n",
    "  - **intercept** `b` → expected margin at **0%** discount\n",
    "  - **break‑even discount** `d* = −b / a` → where expected margin hits **0**\n",
    "  - **margin drop per +5pp/+10pp** → slope scaled by 0.05 / 0.10\n",
    "- We keep results even if `d*` falls outside [0,1] (that tells us break‑even is beyond typical discount ranges).\n",
    "- Exported to `reports/discount_elasticity_by_subcat.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def fit_slope_intercept(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Fit y = a*x + b via OLS (numpy.polyfit) and return (a, b, r2).\"\"\"\n",
    "    a, b = np.polyfit(x, y, 1)  # unweighted; could add weights via 'w=' if desired\n",
    "    yhat = a*x + b\n",
    "    ss_res = np.sum((y - yhat)**2)\n",
    "    ss_tot = np.sum((y - y.mean())**2)\n",
    "    r2 = 1 - ss_res/ss_tot if ss_tot != 0 else np.nan\n",
    "    return a, b, r2\n",
    "\n",
    "def elasticity_with_break_even(df: pd.DataFrame, min_rows:int=100, min_unique:int=5) -> pd.DataFrame:\n",
    "    base = df.copy()\n",
    "    base = base[base[\"Sales\"] > 0].copy()\n",
    "    base[\"margin\"] = np.where(base[\"Sales\"] != 0, base[\"Profit\"]/base[\"Sales\"], np.nan)\n",
    "    base = base.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"Discount\",\"margin\"])\n",
    "\n",
    "    rows = []\n",
    "    for subcat, part in base.groupby(\"Sub-Category\"):\n",
    "        if len(part) < min_rows or part[\"Discount\"].nunique() < min_unique:\n",
    "            continue\n",
    "        x = part[\"Discount\"].to_numpy()\n",
    "        y = part[\"margin\"].to_numpy()\n",
    "        a, b, r2 = fit_slope_intercept(x, y)  # slope, intercept, r2\n",
    "        dstar = -b/a if a != 0 else np.nan  # break-even discount\n",
    "        rows.append({\n",
    "            \"Sub-Category\": subcat,\n",
    "            \"slope\": a,\n",
    "            \"intercept\": b,\n",
    "            \"r2\": r2,\n",
    "            \"n\": len(part),\n",
    "            \"d_star\": dstar,\n",
    "            \"d_star_clipped_0_1\": np.clip(dstar, 0, 1) if np.isfinite(dstar) else np.nan,\n",
    "            \"margin_drop_per_5pp\": a * 0.05,\n",
    "            \"margin_drop_per_10pp\": a * 0.10,\n",
    "            \"slope_positive\": bool(a > 0)\n",
    "        })\n",
    "    out = (pd.DataFrame(rows)\n",
    "           .sort_values([\"slope\",\"r2\"], ascending=[True, False])  # most negative slopes first\n",
    "           .reset_index(drop=True))\n",
    "    return out\n",
    "\n",
    "elasticity_be = elasticity_with_break_even(orders_clean, min_rows=100, min_unique=5)\n",
    "\n",
    "# Pretty view + export\n",
    "display_cols = [\"Sub-Category\",\"slope\",\"intercept\",\"r2\",\"n\",\"margin_drop_per_5pp\",\"margin_drop_per_10pp\",\"d_star\",\"d_star_clipped_0_1\"]\n",
    "display(elasticity_be[display_cols].head(15))\n",
    "\n",
    "REPORTS = Path(\"reports\"); REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "elasticity_be.to_csv(REPORTS / \"discount_elasticity_by_subcat.csv\", index=False)\n",
    "print(\"Saved:\", (REPORTS / \"discount_elasticity_by_subcat.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024dde1",
   "metadata": {},
   "source": [
    "## 10) Customer acquisition cohorts & retention\n",
    "\n",
    "**How it’s built**\n",
    "- Cohort = customer’s **first order month**.  \n",
    "- `months_since_cohort` measures elapsed months for each subsequent order.  \n",
    "- Pivot to get a **retention matrix** (0..12 months).\n",
    "\n",
    "**Interpreting the table**\n",
    "- Row 0 shows cohort size.  \n",
    "- Decay shape across columns indicates retention strength.  \n",
    "- Compare cohorts to see if newer cohorts retain better/worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d127531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfc = orders_clean.dropna(subset=[\"Customer ID\",\"Order ID\",\"Order Date\"]).copy()\n",
    "dfc[\"Order Date\"] = pd.to_datetime(dfc[\"Order Date\"])\n",
    "dfc[\"cohort\"] = dfc.groupby(\"Customer ID\")[\"Order Date\"].transform(lambda s: s.min().to_period(\"M\").to_timestamp())\n",
    "dfc[\"period\"] = dfc[\"Order Date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "dfc[\"months_since_cohort\"] = ((dfc[\"period\"].dt.year - dfc[\"cohort\"].dt.year) * 12\n",
    "                              + (dfc[\"period\"].dt.month - dfc[\"cohort\"].dt.month))\n",
    "\n",
    "active = (dfc.groupby([\"cohort\",\"months_since_cohort\"])[\"Customer ID\"]\n",
    "              .nunique().rename(\"active_customers\").reset_index())\n",
    "cohort_size = active[active[\"months_since_cohort\"]==0][[\"cohort\",\"active_customers\"]].rename(columns={\"active_customers\":\"cohort_size\"})\n",
    "ret = active.merge(cohort_size, on=\"cohort\", how=\"left\")\n",
    "ret[\"retention\"] = ret[\"active_customers\"] / ret[\"cohort_size\"]\n",
    "ret_pivot = ret.pivot(index=\"cohort\", columns=\"months_since_cohort\", values=\"retention\").fillna(0.0)\n",
    "ret_pivot.iloc[:,:13].round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0670c6",
   "metadata": {},
   "source": [
    "## 11) Operations — lead times & on-time performance\n",
    "\n",
    "**KPIs**\n",
    "- SLA = 5 days on-time definition.  \n",
    "- Mean lead time, **P95** lead time, on-time %, and unique orders by `Ship Mode`.\n",
    "\n",
    "**Why P95**\n",
    "- More robust than max; reflects “typical worst case” experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ops = orders_clean[[\"Order ID\",\"Ship Mode\",\"Ship Lead Time (days)\"]].dropna()\n",
    "sla_days = 5\n",
    "ops[\"on_time\"] = (ops[\"Ship Lead Time (days)\"] <= sla_days).astype(int)\n",
    "\n",
    "ops_summary = (ops.groupby(\"Ship Mode\", as_index=False)\n",
    "                 .agg(avg_lead=(\"Ship Lead Time (days)\",\"mean\"),\n",
    "                      p95_lead=(\"Ship Lead Time (days)\", lambda s: np.percentile(s,95)),\n",
    "                      on_time_rate=(\"on_time\",\"mean\"),\n",
    "                      orders=(\"Order ID\",\"nunique\"))\n",
    "                 .sort_values(\"on_time_rate\", ascending=False))\n",
    "ops_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed45c3",
   "metadata": {},
   "source": [
    "## 12) Star-schema exports for BI\n",
    "\n",
    "**What we export**\n",
    "- **fact_order_lines.csv** (all analysis fields, including Returned/Rep/Lead-Time/Margin/Year/Month).  \n",
    "- Dims: **dim_customer**, **dim_product**, **dim_region**, **dim_market** (deduplicated keys).\n",
    "\n",
    "**How to model in Power BI**\n",
    "- `fact_order_lines` joins to each dim on its key (`Customer ID`, `Product ID`, `Region`, `Market`).  \n",
    "- Build simple measures (Sales, Profit, Return Rate) over the fact; slice by dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b672787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "DATA_OUT = Path(\"data/processed\"); DATA_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fact_cols = [\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Customer ID\",\"Customer Name\",\"Segment\",\n",
    "             \"City\",\"State\",\"Country\",\"Postal Code\",\"Market\",\"Region\",\"Product ID\",\"Category\",\"Sub-Category\",\n",
    "             \"Product Name\",\"Sales\",\"Quantity\",\"Discount\",\"Profit\",\"Shipping Cost\",\"Order Priority\",\n",
    "             \"Returned\",\"Person\",\"Ship Lead Time (days)\",\"Profit Margin\",\"Year\",\"Month\"]\n",
    "fact = orders_clean[fact_cols].copy()\n",
    "fact.to_csv(DATA_OUT / \"fact_order_lines.csv\", index=False)\n",
    "\n",
    "dim_customer = (orders_clean[[\"Customer ID\",\"Customer Name\",\"Segment\",\"City\",\"State\",\"Country\",\"Postal Code\"]]\n",
    "                .drop_duplicates().sort_values(\"Customer ID\"))\n",
    "dim_product  = (orders_clean[[\"Product ID\",\"Category\",\"Sub-Category\",\"Product Name\"]]\n",
    "                .drop_duplicates().sort_values(\"Product ID\"))\n",
    "dim_region   = (orders_clean[[\"Region\",\"Person\"]].drop_duplicates().sort_values(\"Region\"))\n",
    "dim_market   = (orders_clean[[\"Market\"]].drop_duplicates().sort_values(\"Market\"))\n",
    "\n",
    "dim_customer.to_csv(DATA_OUT / \"dim_customer.csv\", index=False)\n",
    "dim_product.to_csv(DATA_OUT / \"dim_product.csv\", index=False)\n",
    "dim_region.to_csv(DATA_OUT / \"dim_region.csv\", index=False)\n",
    "dim_market.to_csv(DATA_OUT / \"dim_market.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\", [str(DATA_OUT / n) for n in [\n",
    "    \"fact_order_lines.csv\",\"dim_customer.csv\",\"dim_product.csv\",\"dim_region.csv\",\"dim_market.csv\"\n",
    "]])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
